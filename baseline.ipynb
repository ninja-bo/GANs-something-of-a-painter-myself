{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonetPhotoDataset(Dataset):\n",
    "    def __init__(self, monet_dir, photo_dir, transform=None):\n",
    "        self.monet_images = [os.path.join(monet_dir, img) for img in os.listdir(monet_dir) if img.endswith('.jpg')]\n",
    "        self.photo_images = [os.path.join(photo_dir, img) for img in os.listdir(photo_dir) if img.endswith('.jpg')]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.monet_images), len(self.photo_images))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        monet_img = Image.open(self.monet_images[idx]).convert(\"RGB\")\n",
    "        photo_img = Image.open(self.photo_images[idx]).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            monet_img = self.transform(monet_img)\n",
    "            photo_img = self.transform(photo_img)\n",
    "\n",
    "        return monet_img, photo_img\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "monet_dir = './data/monet_jpg/'\n",
    "photo_dir = './data/photo_jpg/'\n",
    "\n",
    "dataset = MonetPhotoDataset(monet_dir, photo_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "#### Comparison\n",
    "##### 1. standard GAN\n",
    "In a standard GAN, there are two main components:\n",
    "- Generator (G): This model generates new data instances (e.g., images) that resemble the training data. Its goal is to create data that the discriminator cannot distinguish from real data.\n",
    "- Discriminator (D): This model evaluates the data produced by the generator against real data. Its goal is to correctly classify data as either \"real\" (from the training set) or \"fake\" (generated by G).\n",
    "\n",
    "##### 2. CycleGAN\n",
    "- Two Generators:\n",
    "  - Generator G: Transforms images from domain X (e.g., photos) to domain Y (e.g., Monet-style paintings).\n",
    "  - Generator F: Transforms images from domain Y (Monet-style paintings) back to domain X (photos).\n",
    "- Two Discriminators:\n",
    "  - Discriminator D_X: Tries to distinguish between real images from domain X and fake images generated by F.\n",
    "  - Discriminator D_Y: Tries to distinguish between real images from domain Y and fake images generated by G.\n",
    "- Cycle Consistency Loss:\n",
    "  - To ensure that the transformation is meaningful, CycleGAN introduces the concept of cycle consistency. This means if you transform an image to the other domain and back again, you should get the original image. \n",
    "- Identity Loss (Optional but often used):This loss ensures that if you input an image from one domain into the generator corresponding to that domain, it should produce the same image. \n",
    "\n",
    "\n",
    "#### Summary\n",
    "The code you've been working on implements a CycleGAN, which is a specific type of GAN designed for unpaired image-to-image translation. Unlike a standard GAN, which might generate new images from random noise, a CycleGAN is focused on transforming images from one domain to another (e.g., turning photos into Monet-style paintings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Initial convolution block\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, stride=1, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Downsampling\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(256) for _ in range(9)])\n",
    "        # Upsampling\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Output layer\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, 7, stride=1, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        def discriminator_block(in_filters, out_filters, stride=2, normalize=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=stride, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(3, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100] Batch [0/300] G Loss: 11.8309, F Loss: 9.9742, Dx Loss: 0.5049, Dy Loss: 0.6386\n",
      "Epoch [0/100] Batch [100/300] G Loss: 5.6284, F Loss: 4.8153, Dx Loss: 0.2371, Dy Loss: 0.2183\n",
      "Epoch [0/100] Batch [200/300] G Loss: 6.3874, F Loss: 5.2291, Dx Loss: 0.1397, Dy Loss: 0.3567\n",
      "Epoch [1/100] Batch [0/300] G Loss: 4.2739, F Loss: 3.6884, Dx Loss: 0.3393, Dy Loss: 0.2366\n",
      "Epoch [1/100] Batch [100/300] G Loss: 5.3573, F Loss: 4.1936, Dx Loss: 0.1757, Dy Loss: 0.1648\n",
      "Epoch [1/100] Batch [200/300] G Loss: 3.4330, F Loss: 4.4492, Dx Loss: 0.1828, Dy Loss: 0.1770\n",
      "Epoch [2/100] Batch [0/300] G Loss: 5.8017, F Loss: 4.9839, Dx Loss: 0.1453, Dy Loss: 0.0721\n",
      "Epoch [2/100] Batch [100/300] G Loss: 4.0471, F Loss: 3.5777, Dx Loss: 0.2761, Dy Loss: 0.2122\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator_g = Generator().to(device)\n",
    "generator_f = Generator().to(device)\n",
    "discriminator_x = Discriminator().to(device)\n",
    "discriminator_y = Discriminator().to(device)\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = torch.optim.Adam(generator_g.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "f_optimizer = torch.optim.Adam(generator_f.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "dx_optimizer = torch.optim.Adam(discriminator_x.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "dy_optimizer = torch.optim.Adam(discriminator_y.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss functions\n",
    "criterion_gan = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "# Training\n",
    "epochs = 100\n",
    "lambda_cycle = 10.0\n",
    "lambda_identity = 5.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (monet, photo) in enumerate(dataloader):\n",
    "        monet, photo = monet.to(device), photo.to(device)\n",
    "\n",
    "        # ----------- Train Generators --------------\n",
    "        # Generator g\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        fake_monet = generator_g(photo)\n",
    "        cycled_photo = generator_f(fake_monet)\n",
    "        same_monet = generator_g(monet)\n",
    "        \n",
    "        # Losses for generator g\n",
    "        identity_loss_g = criterion_identity(same_monet, monet) * lambda_identity\n",
    "        gan_loss_g = criterion_gan(discriminator_y(fake_monet), torch.ones_like(discriminator_y(fake_monet)))\n",
    "        cycle_loss_g = criterion_cycle(cycled_photo, photo) * lambda_cycle\n",
    "\n",
    "        total_g_loss = identity_loss_g + gan_loss_g + cycle_loss_g\n",
    "        total_g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Generator f\n",
    "        f_optimizer.zero_grad()\n",
    "\n",
    "        fake_photo = generator_f(monet)\n",
    "        cycled_monet = generator_g(fake_photo)\n",
    "        same_photo = generator_f(photo)\n",
    "        \n",
    "        # Losses for generator f\n",
    "        identity_loss_f = criterion_identity(same_photo, photo) * lambda_identity\n",
    "        gan_loss_f = criterion_gan(discriminator_x(fake_photo), torch.ones_like(discriminator_x(fake_photo)))\n",
    "        cycle_loss_f = criterion_cycle(cycled_monet, monet) * lambda_cycle\n",
    "\n",
    "        total_f_loss = identity_loss_f + gan_loss_f + cycle_loss_f\n",
    "        total_f_loss.backward()\n",
    "        f_optimizer.step()\n",
    "\n",
    "        # ----------- Train Discriminators --------------\n",
    "        # Discriminator x\n",
    "        dx_optimizer.zero_grad()\n",
    "\n",
    "        pred_real_monet = discriminator_x(monet)\n",
    "        pred_fake_monet = discriminator_x(fake_photo.detach())\n",
    "        \n",
    "        dx_loss_real = criterion_gan(pred_real_monet, torch.ones_like(pred_real_monet))\n",
    "        dx_loss_fake = criterion_gan(pred_fake_monet, torch.zeros_like(pred_fake_monet))\n",
    "        \n",
    "        dx_loss = (dx_loss_real + dx_loss_fake) * 0.5\n",
    "        dx_loss.backward()\n",
    "        dx_optimizer.step()\n",
    "\n",
    "        # Discriminator y\n",
    "        dy_optimizer.zero_grad()\n",
    "\n",
    "        pred_real_photo = discriminator_y(photo)\n",
    "        pred_fake_photo = discriminator_y(fake_monet.detach())\n",
    "        \n",
    "        dy_loss_real = criterion_gan(pred_real_photo, torch.ones_like(pred_real_photo))\n",
    "        dy_loss_fake = criterion_gan(pred_fake_photo, torch.zeros_like(pred_fake_photo))\n",
    "        \n",
    "        dy_loss = (dy_loss_real + dy_loss_fake) * 0.5\n",
    "        dy_loss.backward()\n",
    "        dy_optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch [{epoch}/{epochs}] Batch [{i}/{len(dataloader)}] '\n",
    "                  f'G Loss: {total_g_loss.item():.4f}, F Loss: {total_f_loss.item():.4f}, '\n",
    "                  f'Dx Loss: {dx_loss.item():.4f}, Dy Loss: {dy_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator_g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (_, photo) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      8\u001b[0m     photo \u001b[38;5;241m=\u001b[39m photo\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m     fake_monet \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator_g\u001b[49m(photo)\n\u001b[1;32m     11\u001b[0m     save_image(fake_monet, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonet_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m:  \u001b[38;5;66;03m# Limiting to 10,000 images as per competition rules\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generator_g' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "output_dir = \"generated_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, (_, photo) in enumerate(dataloader):\n",
    "    photo = photo.to(device)\n",
    "    fake_monet = generator_g(photo)\n",
    "    \n",
    "    save_image(fake_monet, os.path.join(output_dir, f\"monet_{i}.jpg\"))\n",
    "    if i >= 10000:  # Limiting to 10,000 images as per competition rules\n",
    "        break\n",
    "\n",
    "# Zipping the images\n",
    "import shutil\n",
    "shutil.make_archive(\"images\", 'zip', output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
